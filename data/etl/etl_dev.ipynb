{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import uuid\n",
    "from frontmatter import Frontmatter\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "PROJECT_ROOT = os.getenv(\"PROJECT_ROOT\")\n",
    "RAW_DATA_SOURCE_PATH = os.getenv(\"RAW_DATA_SOURCE_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract\n",
    "* manually cloned azure-docs repo\n",
    "* saved to RAW_DATA_SOURCE_PATH  \n",
    "  \n",
    "```bash\n",
    "> git clone --branch main --single-banch --depth 1 https://github.com/MicrosoftDocs/azure-docs.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform\n",
    "* find all md files in source dir\n",
    "* process each doc \n",
    "  * metadata\n",
    "  * full doc -> [sections] -> [[chunks]]  \n",
    "  \n",
    "* _input_: str - path to raw data (_n_ = 17423)  \n",
    "* _output_: list - json of all docs (_n_ = 17281)  \n",
    "  \n",
    "**schema**\n",
    "```json  \n",
    "{  \n",
    "    \"id\": str,  // Unique identifier for the document  \n",
    "     \"filename\": str,  // Name of the markdown file  \n",
    "     \"path\": str,  // Full file path of the markdown file  \n",
    "     \"title\": str,  // Title of the document, extracted from frontmatter   or content\n",
    "     \"metadata\": {  // Additional metadata extracted from the frontmatter  \n",
    "          \"description\": str,  // Brief description of the document  \n",
    "          \"ms.date\": str,  // Date associated with the document  \n",
    "          \"ms.topic\": str  // Topic category of the document  \n",
    "     },\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"section_content\": str,  // Doc section (split by `# `)\n",
    "            \"chunks\": list  // Section split into chunks of ~200 words\n",
    "        },\n",
    "    ]\n",
    "}  \n",
    "```\n",
    "\n",
    "_example_\n",
    "```json\n",
    "{\n",
    "    \"id\": \"xx0108gaGG-089dg-4JGon-98sgG3l\",\n",
    "    \"filename\": \"set-up-sso.md\",\n",
    "    \"path\": \"full/path/to/doc.md\",\n",
    "    \"title\": \"Set up single sign-on for Microsoft Defender for IoT sensor console\",\n",
    "    \"metadata\": {\n",
    "        \"description\": \"Learn how to set up single si...\",\n",
    "        \"ms.date\": \"04/10/2024\",\n",
    "        \"ms.topic\": \"how-to\",\n",
    "        },\n",
    "    \"sections\": [\n",
    "        {\n",
    "            \"section_content\": \"# Set up single sign-on for the sensor console\\n\\nIn this article, you learn how to set up single sign-on (SSO) for the Defender for IoT sensor console using Microsoft Entra ID. With SSO, your organization\\'s users can simply sign into the sensor console, and don\\'t need multiple login credentials across different sensors and sites. \\n\\nUsi\",\n",
    "            \"chunks\": [\"chunk 1\", \"chunk 2\", \"chunk n\"]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_md_files(path):\n",
    "    md_files = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                md_files.append(os.path.join(root, file))\n",
    "    return md_files\n",
    "\n",
    "\n",
    "all_md_files = find_md_files(RAW_DATA_SOURCE_PATH)\n",
    "all_md_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_md_files(\n",
    "    all_md_filepaths: List[str], chunk_size: int = 200\n",
    ") -> List[dict]:\n",
    "    \"\"\"Process all markdown files into a list of indexable documents.\"\"\"\n",
    "    processed_files = []\n",
    "    for file_path in all_md_filepaths:\n",
    "        processed_file = process_markdown_file(file_path, chunk_size)\n",
    "        if processed_file:\n",
    "            processed_files.append(processed_file)\n",
    "    return processed_files\n",
    "\n",
    "\n",
    "def process_markdown_file(file_path: str, chunk_size: int = 200) -> dict:\n",
    "    \"\"\"Process a markdown file into an indexable document.\"\"\"\n",
    "    try:\n",
    "        post = Frontmatter.read_file(file_path)\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        try:\n",
    "            title = post[\"attributes\"][\"title\"]\n",
    "        except:\n",
    "            title = \"Untitled\"\n",
    "\n",
    "        full_doc = post[\"body\"]\n",
    "        doc_sections = split_into_sections(full_doc)\n",
    "        processed_sections = []\n",
    "        for section in doc_sections:\n",
    "            processed_sections.append(\n",
    "                {\n",
    "                    \"section_content\": section,\n",
    "                    \"chunks\": chunk_section(section, chunk_size),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"id\": doc_id,\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"path\": file_path,\n",
    "            \"title\": title,\n",
    "            \"sections\": processed_sections,\n",
    "            \"metadata\": {k: v for k, v in post[\"attributes\"].items() if k != \"title\"},\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def split_into_sections(content: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the full markdown content into sections.\n",
    "    Sections are determined by headers starting with \"# \".\n",
    "    Any text before the first header is treated as its own section.\n",
    "    \"\"\"\n",
    "    # Use regex to split content at positions where a line starts with \"# \"\n",
    "    # The regex uses a lookahead to keep the header in the result.\n",
    "    sections = re.split(r\"(?m)^(?=# )\", content)\n",
    "    return [section.strip() for section in sections if section.strip()]\n",
    "\n",
    "\n",
    "def chunk_section(section: str, chunk_size: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Breaks a section into chunks of approximately `chunk_size` words.\n",
    "    Returns a list of string chunks.\n",
    "    \"\"\"\n",
    "    words = section.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = \" \".join(words[i : i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = process_all_md_files(all_md_files, chunk_size=200)\n",
    "processed_doc = processed_docs[0]\n",
    "print(f\"Processed {len(processed_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"schema: {processed_doc.keys()}\\n\")\n",
    "print(f\"metadata: {processed_doc[\"metadata\"].keys()}\\n\")\n",
    "print(f\"sections: {processed_doc[\"sections\"][0].keys()}\\n\")\n",
    "print(f\"first chunk: {(processed_doc[\"sections\"][0][\"chunks\"][:1])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load\n",
    "* index azure docs to elasticsearch index\n",
    "* save as volume  \n",
    "  \n",
    "**schema**\n",
    "```json\n",
    "{\n",
    "  \"id\": \"keyword\",  \n",
    "  \"filename\": \"keyword\",  \n",
    "  \"path\": \"keyword\",  \n",
    "  \"title\": {\n",
    "      \"type\": \"text\",\n",
    "      \"fields\": {\n",
    "          \"raw\": { \"type\": \"keyword\" }\n",
    "      }\n",
    "  },\n",
    "  \"content\": \"text\",  \n",
    "  \"metadata\": {  \n",
    "       \"description\": \"text\",  \n",
    "       \"ms.date\": { \"type\": \"date\" },  \n",
    "       \"ms.topic\": \"keyword\"  \n",
    "  },\n",
    "  \"sections\": {\n",
    "       \"type\": \"nested\", \n",
    "       \"properties\": {\n",
    "            \"section_content\": \"text\",  \n",
    "            \"chunks\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "       }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
